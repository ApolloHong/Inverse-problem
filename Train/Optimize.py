#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# @Time    : 2023/4/19
# @Author  : 'Lizhan Hong'

import matplotlib.pyplot as plt
import numpy as np
import joblib
import pandas as pd
import scipy
import time
from sko.PSO import PSO
from sko.SA import SAFast, SABoltzmann, SACauchy
from sko.DE import DE
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from scipy.optimize import differential_evolution
from sklearn.svm import SVC
from scipy.optimize import differential_evolution
from OptimaizeSub import *
from ProSub import *
from CS import *


def ex1():
    # Define the fitness function to be optimized
    def fitness(x):
        return x**2

    # Set the algorithm parameters
    bounds = [(-5, 5) for i in range(10)]
    maxiter = 1000
    strategy = 'best1bin'
    popsize = 50

    # Call the Cuckoo Search algorithm
    result = differential_evolution(fitness, bounds, strategy=strategy, maxiter=maxiter, popsize=popsize)

    print("Best solution found: ", result.x)
    print("Fitness value: ", result.fun)

# the handwritten de
def DeOurs(lower_bound = -5, upper_bound = 5, fitnessfunction=fitness_sphere, mutation = de_rand_1_bin_operator):
    # Set the algorithm parameters
    pop_size = 50
    dim = 10
    max_iter = 1000
    F = 0.8
    cr = 0.7
    alpha = 0.1
    lambda_ = 1.5
    # def fitnessfunction():
    #     pass


    # Initialize the population with random solutions
    pop = np.random.uniform(low=lower_bound, high=upper_bound, size=(pop_size, dim))

    # Evaluate the fitness of each solution in the population
    fitness_values = np.array([fitnessfunction(x) for x in pop])

    # Main loop of the algorithm
    for i in range(max_iter):
        # Generate a new population by applying the DE and CS operators
        new_pop = np.zeros((pop_size, dim))
        for j in range(pop_size):
            # Select three solutions(index) from the population at random
            idxs = random.sample(range(pop_size), 3)
            x1, x2, x3 = pop[idxs]
            # Generate a new solution using the DE operator
            v = mutation(x1, x2, x3, F)
            # Apply the CS operator to the new solution
            u = cs_operator(v, alpha, lambda_, lower_bound, upper_bound)
            # Replace one solution in the population if the new solution is better
            if fitnessfunction(u) < fitness_values[idxs[0]]:
                new_pop[j] = u
                fitness_values[idxs[0]] = fitnessfunction(u)
            else:
                new_pop[j] = pop[idxs[0]]
        pop = new_pop

        # Update the population with new solutions generated by Lévy flights
        for j in range(pop_size):
            # Generate a new solution using Lévy flight
            new_solution = cs_operator(pop[j], alpha, lambda_, lower_bound, upper_bound)
            # Replace the solution in the population if the new solution is better
            if fitnessfunction(new_solution) < fitness_values[j]:
                pop[j] = new_solution
                fitness_values[j] = fitnessfunction(new_solution)

    # Return the best solution found
    best_idx = np.argmin(fitness_values)
    best_solution = pop[best_idx]
    print("Best solution found: ", best_solution)
    print("Fitness value: ", fitness_values[best_idx])

# old version
def DeSklearnOneSample(nSample, strategy:str, x0 = np.array([5,0,60,295]), r = 50):
    '''

    :param nSample: the index of the specimen we want
    :param strategy: the mutation strategy we use
    :param x0: the anchor we guesse
    :param r: the dimension (mode) of the POD
    :return:
    '''

    # get data
    knn = joblib.load('../Input/knn4.pkl')
    limits = np.array([[0, 615], [0, 2500], [20, 100], [290, 300]])
    # bounds = get_bounds(y, offsets=[20, 100, 15], limits=limits) + [(290, 300)]
    bounds = limits
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    basis = basis.to_numpy()

    # model = Pipeline([('ss', StandardScaler()),
    #                   ('svc', SVC())])

    # generate initial population
    pop0 = initialize_pop(60, bounds, x=x0)

    #
    # obs = observations[[nSample]]
    obs = observations[nSample, :]

    # print(observations.dtype)
    # observation error ( the fitness function , from mu ∈ R^4 to R^1)
    fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)
    # print(fobj(parameters.iloc[nSample]))
    # print(fobj(np.array(parameters.iloc[nSample])))
    # result = differential_evolution(fobj, limits, seed=42, polish=False, maxiter=10, callback=bestX, x0=x0, init=pop0)
    result = differential_evolution(fobj, limits,strategy= strategy, seed=42, polish=True, x0=x0, init=pop0)
    fun, message, nfev, nit, success, x =\
        result.fun, result.message, result.nfev, result.nit, result.success, result.x
    # print(x)
    # print(f'And the correct parameter is' + '\n' +
    #       f' {parameters.iloc[nSample]}')
    print(f'The error of strategy {strategy} is {x - parameters.iloc[nSample]}')

def DeSklearnGlobal(strategy:str,num = 1, x0 = np.array([5,0,60,295]), r = 50):
    '''

    :param strategy: the mutation strategy we use
    :param num: the number of the data we want to calculate.
    :param x0: the anchor we guesse
    :param r: the dimension (mode) of the POD
    :return:
    '''

    # get data
    knn = joblib.load('../Input/knn4.pkl')
    knntest_input = joblib.load('../Input/knntest_input.pkl') # mu
    knntest_output = joblib.load('../Input/knntest_output.pkl') # alpha
    scalingNor = np.loadtxt('../Input/scalingNor.txt')
    numtest = len(knntest_input[:,0])
    limits = np.array([[0, 615], [0, 2500], [20, 100], [290, 300]])
    dim = len(limits)
    limitsNor = np.array([ [limits[_][0]/scalingNor[_,_], limits[_][1]/scalingNor[_,_]] for _ in range(dim)])
    print(limitsNor)
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    basis = basis.to_numpy()
    sensors.to_numpy()
    obs_test = knntest_output @ basis @ sensors.T
    obs_test.to_numpy()


    # make zeroes in shape of the input parameter
    # error = np.zeros( (len(knntest_input[0]) , 1) )

    # create the result data
    columns = ['time', 'st', 'pw', 'bu', 'tn', 'fitnessfunc']
    index = ['the predicted parameter', 'the true data', 'the error']
    results = pd.DataFrame(np.zeros((len(index), len(columns))),
                           columns=columns,
                           index=index)
    for i in range(num):
        obs = obs_test.iloc[i,:]
        obs = np.array(obs)
        # print(obs.shape,observations[i,:].shape)
        mu_true = knntest_input[i,:]
        x0 = mu_true
        # generate initial population
        pop0 = initialize_pop(60, limitsNor, x=x0)

        # begin training
        t0 = time.time()
        # observation error ( the fitness function , from mu ∈ R^4 to R^1)
        fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)
        result = differential_evolution(fobj, limitsNor,strategy= strategy, seed=42, polish=False, x0=x0, init=pop0)
        fun, message, nfev, nit, success, x =\
            result.fun, result.message, result.nfev, result.nit, result.success, result.x

        # inverse_normalize
        x = x @ scalingNor
        mu_true = mu_true @ scalingNor

        # old method
        # mu_true = np.asarray(mu_true).reshape((len(knntest_input[0]) , 1))
        # x = np.asarray(x).reshape((len(knntest_input[0]) , 1))

        t1 = time.time() - t0
        results.iloc[0,0] += t1 / num
        results.iloc[1,0] += t1 / num
        results.iloc[2,0] += t1 / num
        results.iloc[0 , -1] += fun
        results.iloc[1 , -1] += fun
        results.iloc[2 , -1] += fun
        for i in range(4):
            results.iloc[0, i + 1] += x[i] / num
            results.iloc[1, i + 1] += mu_true[i] / num
            results.iloc[2, i + 1] += (x[i] - mu_true[i]) / mu_true[i]

    # ORIGINAL method
    # results = [onesample(i) for i in range(numtest)]
    # resultsError = np.sum(error, axis=1) / num
    # print(f'The error of strategy {strategy} is {resultsError}')
    # return resultsError

    # results.to_csv('../Output/DeTest.txt')
    return results




def PSOglobal(x0 = np.array([5, 0, 60, 295]), r = 50):
    # adjust the parameter
    max_iter = 50
    numtest = 924

    # get data
    knn = joblib.load('../Input/knn4.pkl')
    knntest_input = joblib.load('../Input/knntest_input.pkl') # mu
    knntest_output = joblib.load('../Input/knntest_output.pkl') # alpha
    limits = [[0, 615], [0, 2500], [20, 100], [290, 300]]
    dim = len(limits) # the dim of the parameter
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    basis = basis.to_numpy()
    obs_test = knntest_output @ basis @ sensors.T

    # create the
    columns = ['time', 'st', 'pw', 'bu', 'tn']
    index = ['PrePara', 'the true data', 'the error']
    results = pd.DataFrame(np.zeros((len(index), len(columns))),
                           columns=columns,
                           index=index)



    for _ in range(numtest):
        t0 = time.time()
        obs = observations[_*5, :]
        mu_true = knntest_input[_*5, :]
        fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)

        pso = PSO(func=fobj, n_dim=dim, pop=40, max_iter=max_iter,
                  lb=[limits[_][0] for _ in range(dim)], ub=[limits[_][1] for _ in range(dim)])
        pso.record_mode = True
        pso.run()
        t1 = time.time() - t0
        plt.plot(pso.gbest_y_hist)
        results.iloc[0,0] += t1 / numtest
        results.iloc[1,0] += t1 / numtest
        results.iloc[2,0] += t1 / numtest
        for i in range(4):
            results.iloc[0, i + 1] += pso.gbest_x[i] / numtest
            results.iloc[1, i + 1] += mu_true[i] / numtest
            # results.iloc[2, i + 1] += np.abs(pso.gbest_x[i] - mu_true[i]) / numtest
            results.iloc[2, i + 1] += (pso.gbest_x[i] - mu_true[i]) / numtest
        # print(mu_true)


    # print(results)
    plt.show()
    results.to_csv('../Output/PSOresults.txt')
    # print(t1)
    # print('best_x is ', pso.gbest_x, 'best_y is', pso.gbest_y, 'the true mu is', mu_true)


def SAscikit(x0 = np.array([5,0,60,295]), r = 50):
    # adjust the parameter
    max_iter = 50
    numtest = 1
    strategies = [SAFast,SABoltzmann,SACauchy]

    # get data
    knn = joblib.load('../Input/knn4.pkl')
    knntest_input = joblib.load('../Input/knntest_input.pkl') # mu
    knntest_output = joblib.load('../Input/knntest_output.pkl') # alpha
    limits = [[0, 615], [0, 2500], [20, 100], [290, 300]]
    dim = len(limits) # the dim of the parameter
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    basis = basis.to_numpy()
    obs_test = knntest_output @ basis @ sensors.T

    # create the
    columns = ['time', 'st', 'pw', 'bu', 'tn','Y']
    index = ['Para of SAFast', 'TrueData of SAFast', 'Error of SAFast',
             'Para of SABoltzmann', 'TrueData of SABoltzmann', 'Error of SABoltzmann',
             'Para of SACauchy', 'TrueData of SACauchy', 'Error of SACauchy'
             ]
    # print(index)
    results = pd.DataFrame(np.zeros((len(index), len(columns))),
                           columns=columns,
                           index=index)

    for _ in range(numtest):
        t0 = time.time()
        obs = observations[_*5, :]
        mu_true = knntest_input[_*5, :]
        x0 = mu_true
        fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)


        for j in range(3):
            optimizer = strategies[j](func=fobj, x0=x0, T_max=1, T_min=1e-9, L=300, max_stay_counter=150)
            optimizer.run()
            t1 = time.time() - t0
            results.iloc[0+j*3,0] += t1 / numtest
            results.iloc[1+j*3,0] += t1 / numtest
            results.iloc[2+j*3,0] += t1 / numtest
            results.iloc[0+j*3,-1] += optimizer.best_y
            results.iloc[1+j*3,-1] += optimizer.best_y
            results.iloc[2+j*3,-1] += optimizer.best_y
            for i in range(4):
                results.iloc[0+j*3, i + 1] += optimizer.best_x[i] / numtest
                results.iloc[1+j*3, i + 1] += mu_true[i] / numtest
                # results.iloc[2, i + 1] += np.abs(pso.gbest_x[i] - mu_true[i]) / numtest
                results.iloc[2+j*3, i + 1] += (optimizer.best_x[i] - mu_true[i]) / numtest



    results.to_csv('../Output/SAresults.txt')


def sklearnPipe(nSample,k=4):
    # 导入数据
    input_data_mu = np.loadtxt('../Input/inpower18480_4.txt')
    # standerd = StandardScaler()
    # input_data_mu = standerd.fit(input_data_mu)
    input_data_mu = normalize(input_data_mu)
    input_data_alpha = np.loadtxt(r'../Input/powerIAEA18480coef.txt')
    test_mu = input_data_mu[nSample,:]


    # 划分数据集
    train_input, _, train_output, __ = train_test_split(input_data_mu, input_data_alpha,
                                                                          test_size=0.25, random_state=42,
                                                                          shuffle=True)

    # Define the pipeline steps
    steps = [
        ("ss", StandardScaler()),  # Standardize features
        ("knn", KNeighborsRegressor(n_neighbors=k, weights='distance', p=1, metric='minkowski')),
        ("de", differential_evolution)  # Differential evolution optimization method
    ]

    # Create the pipeline object
    pipeline = Pipeline(steps)

    # Fit the pipeline to the data
    pipeline.fit(train_input, train_output)

    # Predict with the pipeline
    y_pred = pipeline.predict(test_mu)
    print(y_pred)

def FindBestDeStrategy(nSample=15 ):
    strategies = ['best1bin',
            'best1exp',
            'rand1exp',
            'randtobest1exp',
            'currenttobest1exp',
            'best2exp',
            'rand2exp',
            'randtobest1bin',
            'currenttobest1bin',
            'best2bin',
            'rand2bin',
            'rand1bin']
    findbeststrategy = dict(zip(strategies, [DeSklearnGlobal(i) for i in strategies]))
    # print(findbeststrategy)
    with open(r'../Output/DeFindBestStrategyTest.txt') as f:
        f.write(findbeststrategy)
        f.write('\n' + '\n')
    # joblib.dump(findbeststrategy, '../Output/DeFindBestStrategyTest.pkl')
    # findbeststrategy = pd.DataFrame(findbeststrategy)
    # findbeststrategy.to_csv('../Input/DeFindBestStrategy.txt')
    # joblib.dump(findbeststrategy, '../Output/FindBestSklearn10.pkl')

def CSDE(r = 50):

    # get data
    knn = joblib.load('../Input/knn4.pkl')
    knntest_input = joblib.load('../Input/knntest_input.pkl') # mu
    knntest_output = joblib.load('../Input/knntest_output.pkl') # alpha
    limits = [[0, 615], [0, 2500], [20, 100], [290, 300]]
    dim = len(limits) # the dim of the parameter
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    basis = basis.to_numpy()
    # obs_test = knntest_output @ basis @ sensors.T

    # adjust the parameter
    max_iter = 50
    numtest = len(knntest_input[:,0])
    print(numtest)

    # create the
    columns = ['time', 'st', 'pw', 'bu', 'tn', 'fitnessfunc']
    index = ['the predicted parameter', 'the true data', 'the error']
    results = pd.DataFrame(np.zeros((len(index), len(columns))),
                           columns=columns,
                           index=index)



    for _ in range(1):
        t0 = time.time()
        obs = observations[_*5, :]
        mu_true = knntest_input[_*5, :]
        fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)

        best_nest, best_fitness = cuckoo_search(50, dim, fobj, [limits[_][0] for _ in range(dim)],
                                                [limits[_][1] for _ in range(dim)], step_size=0.4)
        t1 = time.time() - t0
        # claculate the mean absolute sum , and numtest is the number of testdata.
        results.iloc[0,0] += t1 / numtest
        results.iloc[1,0] += t1 / numtest
        results.iloc[2,0] += t1 / numtest
        results.iloc[0 , -1] += best_fitness / numtest
        results.iloc[1 , -1] += best_fitness / numtest
        results.iloc[2 , -1] += best_fitness / numtest
        for i in range(4):
            results.iloc[0, i + 1] += best_nest[i] / numtest
            results.iloc[1, i + 1] += mu_true[i] / numtest
            results.iloc[2, i + 1] += (best_nest[i] - mu_true[i]) / mu_true[i]


    results.to_csv('../Output/CSDEresults.txt')


# # 目前废弃
# def DE_CS_algorithm(fitness_func, bounds, pop_size=50, max_iter=1000, F=0.8, cr=0.7, step_size=0.1, pa=0.25, nest_num=50, alpha=1.5):
#     """
#     Implements a hybrid Differential Evolution (DE) and Cuckoo Search (CS) algorithm for optimization problems.
#
#     Parameters:
#     fitness_func (function): the fitness function to be optimized.
#     bounds (tuple): the lower and upper bounds of the decision variables as a tuple of arrays.
#     pop_size (int): the population size for the DE algorithm (default: 50).
#     max_iter (int): the maximum number of iterations for the algorithm (default: 1000).
#     F (float): the scaling factor for the DE algorithm (default: 0.8).
#     cr (float): the crossover rate for the DE algorithm (default: 0.7).
#     step_size (float): the step size for the CS algorithm (default: 0.1).
#     pa (float): the probability of a cuckoo egg being discovered (default: 0.25).
#     nest_num (int): the number of nests for the CS algorithm (default: 50).
#     alpha (float): the power-law index for the Levy flight in the CS algorithm (default: 1.5).
#
#     Returns:
#     A tuple containing the best solution found and its fitness value.
#     """
#
#     # Define the fitness function to be optimized
#     def fitness(x):
#         return np.sum(x ** 2)
#
#     # Define the DE operator to combine two solutions and create a new one
#     def de_operator(x1, x2, x3, F):
#         return x1 + F * (x2 - x3)
#
#     # Define the CS operator to generate a new solution using Lévy flight
#     def cs_operator(x, alpha, lambda_, lower_bound, upper_bound):
#         levy = lambda_ * np.random.standard_cauchy(len(x))
#         new_x = x + alpha * levy / (abs(levy) ** (1 / 2))
#         return np.clip(new_x, lower_bound, upper_bound)
#
#     # Set the bounds for the decision variables
#     lower_bound, upper_bound = bounds
#
#     # Initialize the population for the DE algorithm with random solutions
#     pop = np.random.uniform(lower_bound, upper_bound, (pop_size, len(lower_bound)))
#
#     # Evaluate the fitness of each solution in the population
#     fitness_values = np.array([fitness_func(x) for x in pop])
#
#     # Initialize the nests for the CS algorithm with random solutions
#     nests = np.random.uniform(lower_bound, upper_bound, (nest_num, len(lower_bound)))
#
#     # Evaluate the fitness of each nest in the population
#     nest_fitness = np.array([fitness_func(x) for x in nests])
#
#     # Main loop of the algorithm
#     for i in range(max_iter):
#         # Generate a new population for the DE algorithm by applying the DE operator
#         new_pop = np.zeros_like(pop)
#         for j in range(pop_size):
#             # Select three solutions from the population at random
#             idxs = np.random.choice(pop_size, 3, replace=False)
#             x1, x2, x3 = pop[idxs]
#             # Generate a new solution using the DE operator
#             v = de_operator(x1, x2, x3, F, cr, lower_bound, upper_bound)
#             # Evaluate the fitness of the new solution
#             fv = fitness_func(v)
#             # Choose the best solution between the new solution and the original solution
#             if fv < fitness_values[j]:
#                 new_pop[j] = v
#                 fitness_values[j] = fv
#             else:
#                 new_pop[j] = pop[j]
#         pop = new_pop
#
#         # Generate a new population for the CS algorithm by applying the CS operator
#         new_nests = np.zeros_like(nests)
#         for j, nest in enumerate(nests):
#             # Generate a new solution using the Levy flight in the CS algorithm
#             v = cs_operator(nest, step_size, lower_bound, upper_bound, alpha)
#             # Evaluate the fitness of the new solution
#             fv = fitness_func(v)
#             # Choose the best solution between the new solution and the original solution
#             if fv < nest_fitness[j]:
#                 new_nests[j] = x
#                 nest_fitness[j] = fv
#             else:
#                 new_nests[j] = nest[j]
#
#                 # Sort the nests by fitness value
#             sorted_idxs = np.argsort(nest_fitness)
#             nest = new_nests[sorted_idxs]
#             nest_fitness = nest_fitness[sorted_idxs]
#
#             # Abandon a fraction of the nests and replace them with new ones
#             # num_abandoned = int(abandon_rate * pop_size)
#             num_abandoned = int(pa * pop_size)
#             abandoned_nests = np.random.choice(pop_size, size=num_abandoned, replace=False)
#             for j in abandoned_nests:
#                 nest[j] = np.random.randn(dim)
#                 nest_fitness[j] = fitness(nest[j])
#
#             # Return the best solution found
#         best_idx = np.argmin(nest_fitness)
#         best_solution = nest[best_idx]
#         return best_solution, nest_fitness[best_idx]
#
#     # Set the algorithm parameters
#     pop_size = 50
#     dim = 10
#     max_iter = 100
#     F = 0.8
#     cr = 0.7
#     pa = 0.25
#     abandon_rate = 0.25
#
#     # Initialize the population with random solutions
#     pop = np.random.randn(pop_size, dim)
#
#     # Evaluate the fitness of each solution in the population
#     fitness_values = np.array([fitness(x) for x in pop])
#
#     # Main loop of the algorithm
#     for i in range(max_iter):
#         # Generate a new population by applying the DE and CS operators
#         new_pop = np.zeros((pop_size, dim))
#         for j in range(pop_size):
#             # Select three solutions(index) from the population at random
#             idxs = random.sample(range(pop_size), 3)
#             x1, x2, x3 = pop[idxs]
#             # Generate a new solution using the DE operator
#             v = de_operator(x1, x2, x3, F, cr)
#             # Apply the CS operator to the new solution
#             u = cs_operator(v, pa)
#             # Replace one solution in the population if the new solution is better
#             if fitness(u) < fitness_values[idxs[0]]:
#                 new_pop[j] = u
#                 fitness_values[idxs[0]] = fitness(u)
#             else:
#                 new_pop[j] = pop[idxs[0]]
#         pop = new_pop
#
#         # Run the CS algorithm on the current population
#         best_solution, best_fitness = cuckoo_search(pop, fitness_values, max_iter=10, pa=pa, abandon_rate=abandon_rate)
#
#         # Replace the worst solution in the population if the new solution is better
#         worst_idx = np.argmax(fitness_values)
#         if best_fitness < fitness_values[worst_idx]:
#             pop[worst_idx] = best_solution
#             fitness_values[worst_idx] = best_fitness
#
#     # Return the best solution found
#     best_idx = np.argmin(fitness_values)
#     best_solution = pop[best_idx]
#     print("Best solution found: ", best_solution)
#     print("Fitness value: ", fitness_values[best_idx])

if __name__ == '__main__':
    # # test
    # print(scipy.__version__)

    # data space
    limits = np.array([[0, 615], [0, 2500], [20, 100], [290, 300]])

    # # workspace
    # we can display the full version of the data
    # # 显示所有列
    # pd.set_option('display.max_columns', None)
    # # 显示所有行
    # pd.set_option('display.max_rows', None)
    # print(joblib.load('../Output/DeFindBestStrategyTest.pkl'))


    # DeSklearnGlobal('rand1exp')
    # DeSklearnOneSample(15,'rand1exp')
    # sklearnPipe(15)
    # FindBestDeStrategy()
    # PSOscikitOneSample()
    # SAscikit()
    # CSDE()


