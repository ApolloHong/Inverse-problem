#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# @Time    : 2023/4/19
# @Author  : 'Lizhan Hong'
from typing import Optional, Any

import matplotlib.pyplot as plt
import numpy as np
import joblib
import pandas as pd
import scipy
import time
import swarmlib

from pandas import DataFrame
from sko.PSO import PSO
from sko.SA import SAFast, SABoltzmann, SACauchy
from sko.DE import DE
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from scipy.optimize import differential_evolution
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from scipy.optimize import differential_evolution
from OptimaizeSub import *
from ProSub import *
from CS import *
from CS2 import *
from DE import *
from PSO import *


def ex1():
    # Define the fitness function to be optimized
    def fitness(x):
        return x ** 2

    # Set the algorithm parameters
    bounds = [(-5, 5) for i in range(10)]
    maxiter = 1000
    strategy = 'best1bin'
    popsize = 50

    # Call the Cuckoo Search algorithm
    result = differential_evolution(fitness, bounds, strategy=strategy, maxiter=maxiter, popsize=popsize)

    print("Best solution found: ", result.x)
    print("Fitness value: ", result.fun)

# # the handwritten de
# def DeOurs(lower_bound = -5, upper_bound = 5, fitnessfunction=fitness_sphere, mutation = de_rand_1_bin_operator):
#     # Set the algorithm parameters
#     pop_size = 50
#     dim = 10
#     max_iter = 1000
#     F = 0.8
#     cr = 0.7
#     alpha = 0.1
#     lambda_ = 1.5
#     # def fitnessfunction():
#     #     pass
#
#
#     # Initialize the population with random solutions
#     pop = np.random.uniform(low=lower_bound, high=upper_bound, size=(pop_size, dim))
#
#     # Evaluate the fitness of each solution in the population
#     fitness_values = np.array([fitnessfunction(x) for x in pop])
#
#     # Main loop of the algorithm
#     for i in range(max_iter):
#         # Generate a new population by applying the DE and CS operators
#         new_pop = np.zeros((pop_size, dim))
#         for j in range(pop_size):
#             # Select three solutions(index) from the population at random
#             idxs = random.sample(range(pop_size), 3)
#             x1, x2, x3 = pop[idxs]
#             # Generate a new solution using the DE operator
#             v = mutation(x1, x2, x3, F)
#             # Apply the CS operator to the new solution
#             u = cs_operator(v, alpha, lambda_, lower_bound, upper_bound)
#             # Replace one solution in the population if the new solution is better
#             if fitnessfunction(u) < fitness_values[idxs[0]]:
#                 new_pop[j] = u
#                 fitness_values[idxs[0]] = fitnessfunction(u)
#             else:
#                 new_pop[j] = pop[idxs[0]]
#         pop = new_pop
#
#         # Update the population with new solutions generated by Lévy flights
#         for j in range(pop_size):
#             # Generate a new solution using Lévy flight
#             new_solution = cs_operator(pop[j], alpha, lambda_, lower_bound, upper_bound)
#             # Replace the solution in the population if the new solution is better
#             if fitnessfunction(new_solution) < fitness_values[j]:
#                 pop[j] = new_solution
#                 fitness_values[j] = fitnessfunction(new_solution)
#
#     # Return the best solution found
#     best_idx = np.argmin(fitness_values)
#     best_solution = pop[best_idx]
#     print("Best solution found: ", best_solution)
#     print("Fitness value: ", fitness_values[best_idx])

# old version

def DeSklearnOneSample(nSample, strategy: str, x0=np.array([5, 0, 60, 295]), r=50):
    '''

    :param nSample: the index of the specimen we want
    :param strategy: the mutation strategy we use
    :param x0: the anchor we guesse
    :param r: the dimension (mode) of the POD
    :return:
    '''

    # get data
    knn = joblib.load('../Input/knn4.pkl')
    limits = np.array([[0, 615], [0, 2500], [20, 100], [290, 300]])
    # bounds = get_bounds(y, offsets=[20, 100, 15], limits=limits) + [(290, 300)]
    bounds = limits
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    basis = basis.to_numpy()

    # model = Pipeline([('ss', StandardScaler()),
    #                   ('svc', SVC())])

    # generate initial population
    pop0 = initialize_pop_uni(60, bounds, x=x0)

    #
    # obs = observations[[nSample]]
    obs = observations[nSample, :]

    # print(observations.dtype)
    # observation error ( the fitness function , from mu ∈ R^4 to R^1)
    fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)
    # print(fobj(parameters.iloc[nSample]))
    # print(fobj(np.array(parameters.iloc[nSample])))
    # result = differential_evolution(fobj, limits, seed=42, polish=False, maxiter=10, callback=bestX, x0=x0, init=pop0)
    result = differential_evolution(fobj, limits, strategy=strategy, seed=42, polish=True, x0=x0, init=pop0)
    fun, message, nfev, nit, success, x = \
        result.fun, result.message, result.nfev, result.nit, result.success, result.x
    # print(x)
    # print(f'And the correct parameter is' + '\n' +
    #       f' {parameters.iloc[nSample]}')
    print(f'The error of strategy {strategy} is {x - parameters.iloc[nSample]}')


def DeSklearnGlobal(strategy: str, record: bool, x0=None, teststep: int = 100, criteria: float = 0.000001,
                    polish:bool = False,
                    num: int = 40, r: int = 50):
    '''

    :param strategy: the mutation strategy we use
    :param record: record the result of one try or not
    :param teststep: the step of the test index
    :param criteria: the criteria for detect the divide by zero risk.
    :param num: the number of the data we want to calculate. (we got 4620 data and take every 10 sample)
    :param x0: the anchor we guess
    :param r: the dimension (mode) of the POD
    :return:
    '''

    # load data
    if x0 is None:
        x0 = [1, 1, 1, 1]
    knn = joblib.load('../Input/knn4.pkl')
    knntest_input = joblib.load('../Input/knntest_input.pkl')  # mu
    knntest_output = joblib.load('../Input/knntest_output.pkl')  # alpha
    scalingNor = np.loadtxt('../Input/scalingNor.txt')
    numtest = len(knntest_input[:, 0])
    dim = len(knntest_input[0, :])
    limits = np.array([[0, 615], [0, 2500], [20, 100], [290, 300]])
    limitsNor = np.array([[limits[_][0] / scalingNor[_, _], limits[_][1] / scalingNor[_, _]] for _ in range(dim)])
    print(limitsNor)

    # reconstruct data
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    basis = basis.to_numpy()
    sensors.to_numpy()
    obs_test = knntest_output @ basis @ sensors.T
    obs_test.to_numpy()

    # make zeroes in shape of the input parameter
    # error = np.zeros( (len(knntest_input[0]) , 1) )

    # add noise
    sigmas = [0, 1]

    # create the result data
    columns = ['time', 'st', 'pw', 'bu', 'tn', 'fitnessfunc']
    index = ['the predicted parameter', 'the true data', 'the error']
    results = pd.DataFrame(np.zeros((len(index), len(columns))),
                           columns=columns,
                           index=index)
    sigma = sigmas[0]
    for i in range(num):
        # print(obs_test.shape)    (4620, 84)
        obs = obs_test.iloc[i * teststep, :]
        obs = np.array(obs)
        # print(obs.shape,observations[i,:].shape)
        mu_true = knntest_input[i * teststep, :]
        # x0 = mu_true + np.random.normal(0, sigma/100.0,mu_true.shape) * mu_true
        # generate initial population
        pop0 = initialize_pop_uni(50, limitsNor, x0=x0)

        # begin training
        t0 = time.time()
        # observation error ( the fitness function , from mu ∈ R^4 to R^1)
        fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)
        result = differential_evolution(fobj, limitsNor, strategy=strategy, seed=42, polish=True, x0=x0, init=pop0)
        fun, x = result.fun, result.x

        # inverse_normalize
        x = x @ scalingNor
        mu_true = mu_true @ scalingNor

        # old method
        # mu_true = np.asarray(mu_true).reshape((len(knntest_input[0]) , 1))
        # x = np.asarray(x).reshape((len(knntest_input[0]) , 1))

        t1 = time.time() - t0
        results.iloc[0, 0] += t1 / num
        results.iloc[1, 0] += t1 / num
        results.iloc[2, 0] += t1 / num
        results.iloc[0, -1] += fun
        results.iloc[1, -1] += fun
        results.iloc[2, -1] += fun
        for i in range(4):
            results.iloc[0, i + 1] += x[i] / num
            results.iloc[1, i + 1] += mu_true[i] / num
            # avoid the divide by zero risk
            # if abs(mu_true[i]) < criteria:
            #     # results.iloc[2, i + 1] +=  ( abs(x[i] - mu_true[i]) ) / num
            #     results.iloc[2, i + 1] += 0
            # elif abs(mu_true[i]) >= criteria:
            results.iloc[2, i + 1] += abs(x[i] - mu_true[i]) / num

    # ORIGINAL method
    # results = [onesample(i) for i in range(numtest)]
    # resultsError = np.sum(error, axis=1) / num
    # print(f'The error of strategy {strategy} is {resultsError}')
    # return resultsError

    if not record:
        return results
    else:
        results.to_csv('../Output/DeTest.txt')

def aDeSklearnGlobal(strategy: str, record: bool, n_components_svc, max_iter:int=1000, F = 0.001,
                     x0:list=None, noise=False, test_step: int = 100, initialize_method=initialize_pop_uni,
                     polish:bool = False, criteria: float = 0.000001, num: int = 1, nc: int = 2, r: int = 50):
    """

    :param F: the scaling factor
    :param initialize_method: the initialization method
    :param strategy: the mutation strategy we use
    :param record: record the result of one try or not
    :param max_iter: the maximun of iteration(default=1000)
    :param noise: the kind of noise we use
    :param test_step: the step of the test index
    :param polish: whether to polish or not
    :param criteria: the criteria for detect the divide by zero risk.
    :param num: the number of the data we want to calculate. (we got 4620 data and take every test_step sample)
    :param nc: the number of the principal component in PCA and SVC pipeline
    :param x0: the anchor we guess
    :param r: the dimension (mode) of the POD
    :return:
    """

    # load data
    knn = joblib.load('../Input/knn4.pkl')
    knntest_input = joblib.load('../Input/knntest_input.pkl')  # mu_test
    knntest_output = joblib.load('../Input/knntest_output.pkl')  # alpha_test
    knntrain_input = joblib.load('../Input/knntrain_input.pkl')  # mu_trian
    knntrain_output = joblib.load('../Input/knntrain_output.pkl')  # alpha_train
    models = joblib.load('../Input/SVC_models'+ str(n_components_svc) +'.pkl')
    scalingNor = np.loadtxt('../Input/scalingNor.txt')
    # print(models)

    param_names = ['st','bu','pw','tin']
    limits = np.array([[0, 615], [0, 2500], [20, 100], [290, 300]])
    numtest = len(knntest_input[:, 0])
    numtrain = len(knntrain_input[:,0])
    dim_mu = len(knntest_input[0, :])

    # reconstruct data
    models = [models[param_names[_]] for _ in range(3)]
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    obs_test = knntest_output @ basis @ sensors.T
    obs_train = knntrain_output @ basis @ sensors.T
    field_test = knntest_output @ basis
    knntrain_param = knntrain_input @ scalingNor
    basis = basis.to_numpy()
    sensors.to_numpy()
    obs_test.to_numpy()
    obs_train.to_numpy()

    # add noise
    sigmas = [0, 1]
    sigma = sigmas[0]

    # # train the classifying model
    # # the model we use is the best pipline in the best-para finding process(like
    # # 'Pipeline(steps=[('pca', PCA(n_components=2)), ('ss', StandardScaler()), ('svc', SVC())])')
    # model = Pipeline([('pca', PCA(n_components=2)),
    #                   ('ss', StandardScaler()),
    #                   ('svc', SVC())])
    # models = []
    # for _ in range(len(param_names[0:3])):
    #     mu_train_discrete = discrete_parameters(knntrain_param[:, _], param_names[_])
    #     # reshape the mu_train_discrete with single feature
    #     mu_train_discrete = mu_train_discrete.reshape(-1, 1)
    #     # we use np.ravel to flatten the matrix
    #     model.fit(np.array(obs_train), np.ravel(mu_train_discrete))
    #     models.append(model)

    # create the result data
    columns = ['time', 'st', 'pw', 'bu', 'tn', 'fitnessfunc']
    index = ['the predicted parameter', 'the true data', 'the error']
    results = pd.DataFrame(np.zeros((len(index), len(columns))),
                           columns=columns,
                           index=index)
    for i in range(num):
        # add noise and prepare the data
        obs_test = add_noise(obs_test, sigma)
        obs = obs_test.iloc[i * test_step, :]
        obs = np.array(obs)
        mu_true = knntest_input[i * test_step, :]
        field_i = field_test.iloc[i * test_step, :]

        # get the predicted anchor index
        obs2in = MultiPredictor(models)
        # # reshape obs with single sample
        center_index = obs2in.predict(obs.reshape(1,-1))

        # get the center(anchor) for the first 3 parameters
        anchors = generate_anchors()
        y = to_anchors(np.array(center_index), [anchors['st'], anchors['bu'], anchors['pw']])
        x0 = np.concatenate((y, [295]))  # the last one is the temperature fixed
        bounds = get_bounds_from_anchors(y, offsets=[20, 100, 15], limits=limits) + [(290, 300)]  # add directly to the bound

        # normalise data , limits and bounds
        x0 = x0 @ np.linalg.pinv(scalingNor)  # normalise the data
        limitsNor = np.array([[limits[_][0] / scalingNor[_, _], limits[_][1] / scalingNor[_, _]] for _ in range(dim_mu)])
        boundsNor = np.array([[bounds[_][0] / scalingNor[_, _], bounds[_][1] / scalingNor[_, _]] for _ in range(dim_mu)])

        # generate initial population with the predicted bound
        pop0 = initialize_method(60, boundsNor, x0=x0)

        # begin training
        t0 = time.time()
        # observation error ( the fitness function , from mu ∈ R^4 to R^1)
        fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors) + F * np.linalg.norm(p-x0,ord=2)
        # # field error
        # f_field_L2_error = lambda p: reconstruct_field_error_from_input(p, knn, basis, field_i)
        # f_field_linf_error = lambda p: np.abs(
        #     reconstruct_field_by_inputs(p, knn, basis) - field_i).max() / field_i.max()

        # DE process
        # bestX = DeBest((num, dim_mu), [fobj, f_field_L2_error, f_field_linf_error])
        # result = differential_evolution(fobj, limitsNor, strategy=strategy, seed=42,maxiter=max_iter,callback=bestX,
        #                                 polish=polish, x0=x0, init=pop0)
        result = differential_evolution(fobj, limitsNor, strategy=strategy, seed=42,maxiter=max_iter,
                                        polish=polish, x0=x0, init=pop0)
        fun, x = result.fun, result.x

        # inverse_normalize
        x = x @ scalingNor
        mu_true = mu_true @ scalingNor

        t1 = time.time() - t0
        results.iloc[0, 0] += t1 / num
        results.iloc[1, 0] += t1 / num
        results.iloc[2, 0] += t1 / num
        results.iloc[0, -1] += fun
        results.iloc[1, -1] += fun
        results.iloc[2, -1] += fun
        for i in range(4):
            results.iloc[0, i + 1] += x[i] / num
            results.iloc[1, i + 1] += mu_true[i] / num
            results.iloc[2, i + 1] += abs(x[i] - mu_true[i]) / num

    if not record:
        return results
    else:
        results.to_csv('../Output/aDeTest.txt')

def PSOglobal(x0=np.array([5, 0, 60, 295]), r=50):
    # adjust the parameter
    max_iter = 50
    numtest = 924

    # get data
    knn = joblib.load('../Input/knn4.pkl')
    knntest_input = joblib.load('../Input/knntest_input.pkl')  # mu
    knntest_output = joblib.load('../Input/knntest_output.pkl')  # alpha
    limits = [[0, 615], [0, 2500], [20, 100], [290, 300]]
    dim = len(limits)  # the dim of the parameter
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    basis = basis.to_numpy()
    obs_test = knntest_output @ basis @ sensors.T

    # create the
    columns = ['time', 'st', 'pw', 'bu', 'tn']
    index = ['PrePara', 'the true data', 'the error']
    results = pd.DataFrame(np.zeros((len(index), len(columns))),
                           columns=columns,
                           index=index)

    for _ in range(numtest):
        t0 = time.time()
        obs = observations[_ * 5, :]
        mu_true = knntest_input[_ * 5, :]
        fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)

        pso = PSO(func=fobj, n_dim=dim, pop=40, max_iter=max_iter,
                  lb=[limits[_][0] for _ in range(dim)], ub=[limits[_][1] for _ in range(dim)])
        pso.record_mode = True
        pso.run()
        t1 = time.time() - t0
        plt.plot(pso.gbest_y_hist)
        results.iloc[0, 0] += t1 / numtest
        results.iloc[1, 0] += t1 / numtest
        results.iloc[2, 0] += t1 / numtest
        for i in range(4):
            results.iloc[0, i + 1] += pso.gbest_x[i] / numtest
            results.iloc[1, i + 1] += mu_true[i] / numtest
            # results.iloc[2, i + 1] += np.abs(pso.gbest_x[i] - mu_true[i]) / numtest
            results.iloc[2, i + 1] += (pso.gbest_x[i] - mu_true[i]) / numtest
        # print(mu_true)

    # print(results)
    plt.show()
    results.to_csv('../Output/PSOresults.txt')
    # print(t1)
    # print('best_x is ', pso.gbest_x, 'best_y is', pso.gbest_y, 'the true mu is', mu_true)d


def SAscikit(x0=np.array([5, 0, 60, 295]), r=50):
    # adjust the parameter
    max_iter = 50
    numtest = 1
    strategies = [SAFast, SABoltzmann, SACauchy]

    # get data
    knn = joblib.load('../Input/knn4.pkl')
    knntest_input = joblib.load('../Input/knntest_input.pkl')  # mu
    knntest_output = joblib.load('../Input/knntest_output.pkl')  # alpha
    limits = [[0, 615], [0, 2500], [20, 100], [290, 300]]
    dim = len(limits)  # the dim of the parameter
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    basis = basis.to_numpy()
    obs_test = knntest_output @ basis @ sensors.T

    # create the
    columns = ['time', 'st', 'pw', 'bu', 'tn', 'Y']
    index = ['Para of SAFast', 'TrueData of SAFast', 'Error of SAFast',
             'Para of SABoltzmann', 'TrueData of SABoltzmann', 'Error of SABoltzmann',
             'Para of SACauchy', 'TrueData of SACauchy', 'Error of SACauchy'
             ]
    # print(index)
    results = pd.DataFrame(np.zeros((len(index), len(columns))),
                           columns=columns,
                           index=index)

    for _ in range(numtest):
        t0 = time.time()
        obs = observations[_ * 5, :]
        mu_true = knntest_input[_ * 5, :]
        x0 = mu_true
        fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)

        for j in range(3):
            optimizer = strategies[j](func=fobj, x0=x0, T_max=1, T_min=1e-9, L=300, max_stay_counter=150)
            optimizer.run()
            t1 = time.time() - t0
            results.iloc[0 + j * 3, 0] += t1 / numtest
            results.iloc[1 + j * 3, 0] += t1 / numtest
            results.iloc[2 + j * 3, 0] += t1 / numtest
            results.iloc[0 + j * 3, -1] += optimizer.best_y
            results.iloc[1 + j * 3, -1] += optimizer.best_y
            results.iloc[2 + j * 3, -1] += optimizer.best_y
            for i in range(4):
                results.iloc[0 + j * 3, i + 1] += optimizer.best_x[i] / numtest
                results.iloc[1 + j * 3, i + 1] += mu_true[i] / numtest
                # results.iloc[2, i + 1] += np.abs(pso.gbest_x[i] - mu_true[i]) / numtest
                results.iloc[2 + j * 3, i + 1] += (optimizer.best_x[i] - mu_true[i]) / numtest

    results.to_csv('../Output/SAresults.txt')


def sklearnPipe(nSample, k=4):
    # 导入数据
    input_data_mu = np.loadtxt('../Input/inpower18480_4.txt')
    # standerd = StandardScaler()
    # input_data_mu = standerd.fit(input_data_mu)
    input_data_mu = normalize(input_data_mu)
    input_data_alpha = np.loadtxt(r'../Input/powerIAEA18480coef.txt')
    test_mu = input_data_mu[nSample, :]

    # 划分数据集
    train_input, _, train_output, __ = train_test_split(input_data_mu, input_data_alpha,
                                                        test_size=0.25, random_state=42,
                                                        shuffle=True)

    # Define the pipeline steps
    steps = [
        ("ss", StandardScaler()),  # Standardize features
        ("knn", KNeighborsRegressor(n_neighbors=k, weights='distance', p=1, metric='minkowski')),
        ("de", differential_evolution)  # Differential evolution optimization method
    ]

    # Create the pipeline object
    pipeline = Pipeline(steps)

    # Fit the pipeline to the data
    pipeline.fit(train_input, train_output)

    # Predict with the pipeline
    y_pred = pipeline.predict(test_mu)
    print(y_pred)


def FindBestDeStrategy(savedpathname='DeFindBestStrategyTestKfoldCorrect', nSample=15):
    strategies = ['best1bin',
                  'best1exp',
                  'rand1exp',
                  'randtobest1exp',
                  'currenttobest1exp',
                  'best2exp',
                  'rand2exp',
                  'randtobest1bin',
                  'currenttobest1bin',
                  'best2bin',
                  'rand2bin',
                  'rand1bin']
    findbeststrategy = dict(zip(strategies, [DeSklearnGlobal(i, record=False) for i in strategies]))

    # print(findbeststrategy)
    # with open(r'../Output/DeFindBestStrategyTest.txt') as f:
    #     f.write(findbeststrategy)
    #     f.write('\n' + '\n')

    joblib.dump(findbeststrategy, '../Output/' + savedpathname + '.pkl')

    # findbeststrategy = pd.DataFrame(findbeststrategy)
    # findbeststrategy.to_csv('../Input/DeFindBestStrategy.txt')
    # joblib.dump(findbeststrategy, '../Output/FindBestSklearn10.pkl')


def CSDE(num=1, test_step=1, popsize:int=50, r=50, criteria=0.000001,
         train_SVC:bool=False, confusion_plot:bool=False, initialize_method=initialize_pop_uni, n_components_svc:int=2):
    """

    :param train_SVC:
    :param num: the number of test sample we want to test
    :param test_step: the step of the test index
    :param popsize: the size of the population(number of nests).
    :param r: the dim_mu (mode) of POD
    :param criteria: the criteria for detect the divide by zero risk.
    :param train_SVC: whether to train the SVC or not.
    :return:
    """

    # load data
    knn = joblib.load('../Input/knn4.pkl')
    knntest_input = joblib.load('../Input/knntest_input.pkl')  # mu_test-----normalised
    knntest_output = joblib.load('../Input/knntest_output.pkl')  # alpha_test
    knntrain_input = joblib.load('../Input/knntrain_input.pkl')  # mu_trian-----normalised
    knntrain_output = joblib.load('../Input/knntrain_output.pkl')  # alpha_train
    scalingNor = np.loadtxt('../Input/scalingNor.txt')

    param_names = ['st','bu','pw','tin']
    limits = np.array([[0, 615], [0, 2500], [20, 100], [290, 300]])
    numtest = len(knntest_input[:, 0])
    numtrain = len(knntrain_input[:,0])
    dim_mu = len(knntest_input[0, :])

    # reconstruct data into our physics problem
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    obs_test = knntest_output @ basis @ sensors.T
    obs_train = knntrain_output @ basis @ sensors.T
    field_test = knntest_output @ basis
    knntrain_param = knntrain_input @ scalingNor # ----- inverse_normalised
    knntest_param = knntest_input @ scalingNor # ----- inverse_normalised
    basis = basis.to_numpy()
    sensors.to_numpy()
    obs_test.to_numpy()
    obs_train.to_numpy()

    if train_SVC:
        initial_guess_svc(knntrain_param, knntest_param, param_names, obs_train, obs_test,
                          confusion_plot=confusion_plot,n_components_svc=n_components_svc)
    else:
        # add noise
        sigmas = [0, 1]
        sigma = sigmas[0]

        # read the models
        models = joblib.load('../Input/SVC_models' + str(n_components_svc) + '.pkl')
        models = [models[param_names[_]] for _ in range(3)]
        print(models)

        # # train the classifying model
        # # the model we use is the best pipline in the best-para finding process(like
        # # 'Pipeline(steps=[('pca', PCA(n_components=2)), ('ss', StandardScaler()), ('svc', SVC())])')
        # model = Pipeline([('pca', PCA(n_components=2)),
        #                   ('ss', StandardScaler()),
        #                   ('svc', SVC())])
        # models = []
        # for _ in range(len(param_names[0:3])):
        #     mu_train_discrete = discrete_parameters(knntrain_param[:, _], param_names[_])
        #     # reshape the mu_train_discrete with single feature
        #     mu_train_discrete = mu_train_discrete.reshape(-1, 1)
        #     # we use np.ravel to flatten the matrix
        #     model.fit(np.array(obs_train), np.ravel(mu_train_discrete))
        #     models.append(model)

        # create the results
        columns = ['time', 'st', 'pw', 'bu', 'tn', 'fitnessfunc']
        index = ['the predicted parameter', 'the true data', 'the error']
        results = pd.DataFrame(np.zeros((len(index), len(columns))),
                               columns=columns,
                               index=index)

        for i in range(num):
            # there is a problem , it will always be the dataframe form
            obs = obs_test.iloc[i * test_step, :]
            obs = np.array(obs)
            mu_true = knntest_input[i * test_step, :]
            # get the predicted anchor index
            obs2in = MultiPredictor(models)
            # # reshape obs with single sample
            center_index = obs2in.predict(obs.reshape(1,-1))

            # get the center(anchor) for the first 3 parameters
            anchors = generate_anchors()
            y = to_anchors(np.array(center_index), [anchors['st'], anchors['bu'], anchors['pw']])
            x0 = np.concatenate((y, [295]))  # the last one is the temperature fixed
            print(x0, mu_true)
            bounds = get_bounds_from_anchors(y, offsets=[20, 100, 15], limits=limits) + [(290, 300)]  # add directly to the bound

            # normalise data , limits and bounds
            x0 = x0 @ np.linalg.pinv(scalingNor)  # normalise the data
            limitsNor = np.array([[limits[_][0] / scalingNor[_, _], limits[_][1] / scalingNor[_, _]] for _ in range(dim_mu)])
            boundsNor = np.array([[bounds[_][0] / scalingNor[_, _], bounds[_][1] / scalingNor[_, _]] for _ in range(dim_mu)])

            t0 = time.time()

            # generate initial population with the predicted bound
            pop0 = initialize_method(popsize, boundsNor, x0=x0)

            fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)
            best_nest, best_fitness = cuckoo_search(n=popsize, m=dim_mu, fit_func=fobj, beta=1.5,
                                                    lower_boundary=[limitsNor[_][0] for _ in range(dim_mu)],
                                                    upper_boundary=[limitsNor[_][1] for _ in range(dim_mu)],
                                                    nests_initial=pop0, iter_num=60, step_size=0.4)
            t1 = time.time() - t0

            # inverse_normalize
            best_nest = best_nest @ scalingNor
            mu_true = mu_true @ scalingNor

            # claculate the mean absolute sum , and numtest is the number of testdata.
            results.iloc[0, 0] += t1 / num
            results.iloc[1, 0] += t1 / num
            results.iloc[2, 0] += t1 / num
            results.iloc[0, -1] += best_fitness / num
            results.iloc[1, -1] += best_fitness / num
            results.iloc[2, -1] += best_fitness / num
            for j in range(4):
                results.iloc[0, j + 1] += best_nest[j] / num
                results.iloc[1, j + 1] += mu_true[j] / num
                results.iloc[2, j + 1] += abs(best_nest[j] - mu_true[j]) / num

        results.to_csv('../Output/CSDEresults.txt')


def CS2(num=1):
    # get data
    knn = joblib.load('../Input/knn4.pkl')
    knntest_input = joblib.load('../Input/knntest_input.pkl')  # mu
    knntest_output = joblib.load('../Input/knntest_output.pkl')  # alpha
    scalingNor = np.loadtxt('../Input/scalingNor.txt')
    limits = np.array([[0, 615], [0, 2500], [20, 100], [290, 300]])
    dim = len(limits)
    limitsNor = np.array([(limits[_][0] / scalingNor[_, _], limits[_][1] / scalingNor[_, _]) for _ in range(dim)])
    print(limitsNor)
    parameters, field, observations, sensors = load_power18480()
    basis, _ = load_power18480_pod()
    basis = basis.to_numpy()
    sensors.to_numpy()
    obs_test = knntest_output @ basis @ sensors.T
    obs_test.to_numpy()

    # # adjust the parameter
    # max_iter = 50
    # numtest = len(knntest_input[:,0])
    # print(numtest)

    # create the results
    columns = ['time', 'st', 'pw', 'bu', 'tn', 'fitnessfunc']
    index = ['the predicted parameter', 'the true data', 'the error']
    results = pd.DataFrame(np.zeros((len(index), len(columns))),
                           columns=columns,
                           index=index)

    for i in range(num):
        # there is a problem , it will always be the dataframe form
        obs = obs_test.iloc[i * 10, :]
        obs = np.array(obs)
        mu_true = knntest_input[i * 10, :]

        t0 = time.time()
        fobj = lambda p: reconstruct_observation_error_from_input(p, knn, basis, obs, sensors)

        # best_nest, best_fitness = cuckoo_search(n = 50, m = dim, fit_func=fobj,
        #                                         lower_boundary=[limitsNor[_][0] for _ in range(dim)],
        #                                         upper_boundary=[limitsNor[_][1] for _ in range(dim)],
        #                                         iter_num=60, step_size=0.7)
        CSO(fobj, P=150, n=dim, pa=0.25, beta=1.5, bound=limitsNor,
            plot=False, min=True, verbose=False, Tmax=300).execute()
        t1 = time.time() - t0

        # print(a,b)

        # # inverse_normalize
        # # print(reconstruct_observation_error_from_input(mu_true, knn, basis, obs, sensors))
        # best_nest = best_nest @ scalingNor
        # mu_true = mu_true @ scalingNor
        #
        #
        # # claculate the mean absolute sum , and numtest is the number of testdata.
        # results.iloc[0,0] += t1 / num
        # results.iloc[1,0] += t1 / num
        # results.iloc[2,0] += t1 / num
        # results.iloc[0 , -1] += best_fitness / num
        # results.iloc[1 , -1] += best_fitness / num
        # results.iloc[2 , -1] += best_fitness / num
        # for j in range(4):
        #     results.iloc[0, j + 1] += best_nest[j] / num
        #     results.iloc[1, j + 1] += mu_true[j] / num
        #     # if abs(mu_true[j]) < criteria:
        #     #     # results.iloc[2, j + 1] += ( abs(best_nest[j] - mu_true[j]) ) / num
        #     #     results.iloc[2, i + 1] += 0
        #     # elif abs(mu_true[j]) >= criteria:
        #     results.iloc[2, j + 1] += abs(best_nest[j] - mu_true[j]) / num

    results.to_csv('../Output/CSDEresults.txt')


# # 目前废弃
# def DE_CS_algorithm(fitness_func, bounds, pop_size=50, max_iter=1000, F=0.8, cr=0.7, step_size=0.1, pa=0.25,
#                     nest_num=50, alpha=1.5):
#     """
#     Implements a hybrid Differential Evolution (DE) and Cuckoo Search (CS) algorithm for optimization problems.
#
#     Parameters:
#     fitness_func (function): the fitness function to be optimized.
#     bounds (tuple): the lower and upper bounds of the decision variables as a tuple of arrays.
#     pop_size (int): the population size for the DE algorithm (default: 50).
#     max_iter (int): the maximum number of iterations for the algorithm (default: 1000).
#     F (float): the scaling factor for the DE algorithm (default: 0.8).
#     cr (float): the crossover rate for the DE algorithm (default: 0.7).
#     step_size (float): the step size for the CS algorithm (default: 0.1).
#     pa (float): the probability of a cuckoo egg being discovered (default: 0.25).
#     nest_num (int): the number of nests for the CS algorithm (default: 50).
#     alpha (float): the power-law index for the Levy flight in the CS algorithm (default: 1.5).
#
#     Returns:
#     A tuple containing the best solution found and its fitness value.
#     """
#
#     # Define the fitness function to be optimized
#     def fitness(x):
#         return np.sum(x ** 2)
#
#     # Define the DE operator to combine two solutions and create a new one
#     def de_operator(x1, x2, x3, F):
#         return x1 + F * (x2 - x3)
#
#     # Define the CS operator to generate a new solution using Lévy flight
#     def cs_operator(x, alpha, lambda_, lower_bound, upper_bound):
#         levy = lambda_ * np.random.standard_cauchy(len(x))
#         new_x = x + alpha * levy / (abs(levy) ** (1 / 2))
#         return np.clip(new_x, lower_bound, upper_bound)
#
#     # Set the bounds for the decision variables
#     lower_bound, upper_bound = bounds
#
#     # Initialize the population for the DE algorithm with random solutions
#     pop = np.random.uniform(lower_bound, upper_bound, (pop_size, len(lower_bound)))
#
#     # Evaluate the fitness of each solution in the population
#     fitness_values = np.array([fitness_func(x) for x in pop])
#
#     # Initialize the nests for the CS algorithm with random solutions
#     nests = np.random.uniform(lower_bound, upper_bound, (nest_num, len(lower_bound)))
#
#     # Evaluate the fitness of each nest in the population
#     nest_fitness = np.array([fitness_func(x) for x in nests])
#
#     # Main loop of the algorithm
#     for i in range(max_iter):
#         # Generate a new population for the DE algorithm by applying the DE operator
#         new_pop = np.zeros_like(pop)
#         for j in range(pop_size):
#             # Select three solutions from the population at random
#             idxs = np.random.choice(pop_size, 3, replace=False)
#             x1, x2, x3 = pop[idxs]
#             # Generate a new solution using the DE operator
#             v = de_operator(x1, x2, x3, F)
#             # Evaluate the fitness of the new solution
#             fv = fitness_func(v)
#             # Choose the best solution between the new solution and the original solution
#             if fv < fitness_values[j]:
#                 new_pop[j] = v
#                 fitness_values[j] = fv
#             else:
#                 new_pop[j] = pop[j]
#         pop = new_pop
#
#         # Generate a new population for the CS algorithm by applying the CS operator
#         new_nests = np.zeros_like(nests)
#         for j, nest in enumerate(nests):
#             # Generate a new solution using the Levy-flight in the CS algorithm
#             v = cs_operator(nest, step_size, lower_bound, upper_bound, alpha)
#             # Evaluate the fitness of the new solution
#             fv = fitness_func(v)
#             # Choose the best solution between the new solution and the original solution
#             if fv < nest_fitness[j]:
#                 new_nests[j] = x
#                 nest_fitness[j] = fv
#             else:
#                 new_nests[j] = nest[j]
#
#                 # Sort the nests by fitness value
#             sorted_idxs = np.argsort(nest_fitness)
#             nest = new_nests[sorted_idxs]
#             nest_fitness = nest_fitness[sorted_idxs]
#
#             # Abandon a fraction of the nests and replace them with new ones
#             # num_abandoned = int(abandon_rate * pop_size)
#             num_abandoned = int(pa * pop_size)
#             abandoned_nests = np.random.choice(pop_size, size=num_abandoned, replace=False)
#             for j in abandoned_nests:
#                 nest[j] = np.random.randn(dim)
#                 nest_fitness[j] = fitness(nest[j])
#
#             # Return the best solution found
#         best_idx = np.argmin(nest_fitness)
#         best_solution = nest[best_idx]
#         return best_solution, nest_fitness[best_idx]
#
#     # Set the algorithm parameters
#     pop_size = 50
#     dim = 10
#     max_iter = 100
#     F = 0.8
#     cr = 0.7
#     pa = 0.25
#     abandon_rate = 0.25
#
#     # Initialize the population with random solutions
#     pop = np.random.randn(pop_size, dim)
#
#     # Evaluate the fitness of each solution in the population
#     fitness_values = np.array([fitness(x) for x in pop])
#
#     # Main loop of the algorithm
#     for i in range(max_iter):
#         # Generate a new population by applying the DE and CS operators
#         new_pop = np.zeros((pop_size, dim))
#         for j in range(pop_size):
#             # Select three solutions(index) from the population at random
#             idxs = random.sample(range(pop_size), 3)
#             x1, x2, x3 = pop[idxs]
#             # Generate a new solution using the DE operator
#             v = de_operator(x1, x2, x3, F, cr)
#             # Apply the CS operator to the new solution
#             u = cs_operator(v, pa)
#             # Replace one solution in the population if the new solution is better
#             if fitness(u) < fitness_values[idxs[0]]:
#                 new_pop[j] = u
#                 fitness_values[idxs[0]] = fitness(u)
#             else:
#                 new_pop[j] = pop[idxs[0]]
#         pop = new_pop
#
#         # Run the CS algorithm on the current population
#         best_solution, best_fitness = cuckoo_search(pop, fitness_values, max_iter=10, pa=pa, abandon_rate=abandon_rate)
#
#         # Replace the worst solution in the population if the new solution is better
#         worst_idx = np.argmax(fitness_values)
#         if best_fitness < fitness_values[worst_idx]:
#             pop[worst_idx] = best_solution
#             fitness_values[worst_idx] = best_fitness
#
#     # Return the best solution found
#     best_idx = np.argmin(fitness_values)
#     best_solution = pop[best_idx]
#     print("Best solution found: ", best_solution)
#     print("Fitness value: ", fitness_values[best_idx])


if __name__ == '__main__':
    # parameters
    n_components_svc = 50



    # # test
    # print(scipy.__version__)
    # data space

    # # workspace
    # DeSklearnGlobal('best1bin')
    # FindBestDeStrategy(savedpathname='DeFindBestStrategyTestKfoldCorrectPolished')
    # PSOscikitOneSample()
    # SAscikit()
    # CSDE(train_SVC=True, confusion_plot=True,n_components_svc=n_components_svc)
    aDeSklearnGlobal(strategy='rand2exp', record=True, n_components_svc=n_components_svc, max_iter=100,
                     test_step=100, initialize_method=initialize_pop_uni, polish=False, num=40)
    # CSDE(num=40, test_step=100, train_SVC=False, confusion_plot=False,
    #      n_components_svc=n_components_svc, initialize_method=initialize_pop_uni)
    # CS2()

    # we can display the full version of the data

    # ## find the best strategy for DE
    # # 显示所有列
    # pd.set_option('display.max_columns', None)
    # # 显示所有行
    # pd.set_option('display.max_rows', None)
    # print(joblib.load('../Input/knntrain_input.pkl').shape)
    # print(joblib.load('../Input/knntrain_output.pkl').shape)
    # f = joblib.load('../Output/DeFindBestStrategyTestKfoldCorrect.pkl')
    # Yerror = [FuncError.iloc[:,-1][0] for FuncError in f.values() ]
    # strategies = [key for key in f.keys()]
    # StrategyBest = strategies[Yerror.index(min(Yerror))]
    # print(StrategyBest)
    # print(f)

    # print(np.load('../Output/CSDEresults.txt'))
    # print(pd.read_csv('../Input/split_index_ibest0.txt')['index'].iloc[1])
